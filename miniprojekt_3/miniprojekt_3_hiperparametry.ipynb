{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import notebook as tqdm_notebook\n",
    "\n",
    "from data_functions import *\n",
    "from metric_functions import *\n",
    "from helper_functions import *\n",
    "from models import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"final_data.tar\")\n",
    "train, val = trai_val_split(dataset, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_convs_values = [2, 5, 10, 20]\n",
    "kernel_size_values = [3, 5, 7, 9]\n",
    "stride_values = [1, 2, 3, 5]\n",
    "padding_values = [1, 2, 3, 5]\n",
    "n_channels_values = [16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\", \"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "nets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liczba warstw konwolucyjnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_convs in tqdm_notebook.tqdm(n_convs_values):\n",
    "    n_channels = 10\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    kernel_size = 3\n",
    "    try:\n",
    "        net = CustomNet(num_classes=50, input_size=48, n_convs=n_convs, kernel_size=kernel_size, stride=stride, padding=padding).to(device)\n",
    "    except:\n",
    "        continue\n",
    "    nets[(n_convs, kernel_size, stride, padding)] = net\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True, num_workers=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_hist, train_eval_hist, val_eval_hist = train_model(net, train_loader, val_loader, criterion, optimizer, get_accuracy, device, 25, verbose=False)\n",
    "    model_res = pd.DataFrame([[n_convs, kernel_size, stride, padding, n_channels,loss_hist, train_eval_hist, val_eval_hist, train_eval_hist[-1]-val_eval_hist[-1]]], columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\",\"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "    res = pd.concat([res, model_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for i, n_convs in enumerate(n_convs_values):\n",
    "    iters = [i for i in range(25)]\n",
    "    loss_hist = res[res[\"n_convs\"] == n_convs][\"train_loss\"].values[0]\n",
    "    train_acc = res[res[\"n_convs\"] == n_convs][\"train_acc\"].values[0]\n",
    "    val_acc = res[res[\"n_convs\"] == n_convs][\"val_acc\"].values[0]\n",
    "    plot_training(iters, loss_hist, train_acc, val_acc, axs[i])\n",
    "    axs[i].set_title(f\"n_convs = {n_convs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wraz ze zwiększeniem liczby wartstw konwolucyjnych model robi się przetrenowany\n",
    "- Trzeba polepszyć inne parametry żeby poprawić uogólnianie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_size in tqdm_notebook.tqdm(kernel_size_values):\n",
    "    n_channels = 10\n",
    "    stride = 1\n",
    "    padding = 1\n",
    "    n_convs = 3\n",
    "    try:\n",
    "        net = CustomNet(num_classes=50, input_size=48, n_convs=n_convs, kernel_size=kernel_size, stride=stride, padding=padding).to(device)\n",
    "    except:\n",
    "        continue\n",
    "    nets[(n_convs, kernel_size, stride, padding)] = net\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True, num_workers=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_hist, train_eval_hist, val_eval_hist = train_model(net, train_loader, val_loader, criterion, optimizer, get_accuracy, device, 15, verbose=False)\n",
    "    model_res = pd.DataFrame([[n_convs, kernel_size, stride, padding, n_channels,loss_hist, train_eval_hist, val_eval_hist, train_eval_hist[-1]-val_eval_hist[-1]]], columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\",\"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "    res = pd.concat([res, model_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for i, kernel_size in enumerate(kernel_size_values):\n",
    "    iters = [i for i in range(15)]\n",
    "    loss_hist = res[res[\"kernel_size\"] == kernel_size][\"train_loss\"].values[0]\n",
    "    train_acc = res[res[\"kernel_size\"] == kernel_size][\"train_acc\"].values[0]\n",
    "    val_acc = res[res[\"kernel_size\"] == kernel_size][\"val_acc\"].values[0]\n",
    "    plot_training(iters, loss_hist, train_acc, val_acc, axs[i])\n",
    "    axs[i].set_title(f\"kernel_size = {kernel_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dla mniejszych wartości zbyt przetrenowany\n",
    "- Dla większych bardzo słaba dokładność"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stride in tqdm_notebook.tqdm(stride_values):\n",
    "    n_channels = 10\n",
    "    padding = 1\n",
    "    n_convs = 2\n",
    "    kernel_size = 3\n",
    "    try:\n",
    "        net = CustomNet(num_classes=50, input_size=48, n_convs=n_convs, kernel_size=kernel_size, stride=stride, padding=padding).to(device)\n",
    "    except:\n",
    "        continue\n",
    "    nets[(n_convs, kernel_size, stride, padding)] = net\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True, num_workers=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_hist, train_eval_hist, val_eval_hist = train_model(net, train_loader, val_loader, criterion, optimizer, get_accuracy, device, 15, verbose=False)\n",
    "    model_res = pd.DataFrame([[n_convs, kernel_size, stride, padding, n_channels,loss_hist, train_eval_hist, val_eval_hist, train_eval_hist[-1]-val_eval_hist[-1]]], columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\",\"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "    res = pd.concat([res, model_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for i, stride in enumerate(stride_values):\n",
    "    iters = [i for i in range(15)]\n",
    "    loss_hist = res[res[\"stride\"] == stride][\"train_loss\"].values[0]\n",
    "    train_acc = res[res[\"stride\"] == stride][\"train_acc\"].values[0]\n",
    "    val_acc = res[res[\"stride\"] == stride][\"val_acc\"].values[0]\n",
    "    plot_training(iters, loss_hist, train_acc, val_acc, axs[i])\n",
    "    axs[i].set_title(f\"stride = {stride}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Większy stride daje bardziej stabilne wyniki\n",
    "- Gorsza ogólna dokładność ze zwiększeniem wartości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for padding in tqdm_notebook.tqdm(padding_values):\n",
    "    n_channels = 10\n",
    "    n_convs = 2\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    try:\n",
    "        net = CustomNet(num_classes=50, input_size=48, n_convs=n_convs, kernel_size=kernel_size, stride=stride, padding=padding).to(device)\n",
    "    except:\n",
    "        continue\n",
    "    nets[(n_convs, kernel_size, stride, padding)] = net\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True, num_workers=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_hist, train_eval_hist, val_eval_hist = train_model(net, train_loader, val_loader, criterion, optimizer, get_accuracy, device, 15, verbose=False)\n",
    "    model_res = pd.DataFrame([[n_convs, kernel_size, stride, padding, n_channels,loss_hist, train_eval_hist, val_eval_hist, train_eval_hist[-1]-val_eval_hist[-1]]], columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\",\"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "    res = pd.concat([res, model_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for i, padding in enumerate(padding_values):\n",
    "    iters = [i for i in range(15)]\n",
    "    loss_hist = res[res[\"padding\"] == padding][\"train_loss\"].values[0]\n",
    "    train_acc = res[res[\"padding\"] == padding][\"train_acc\"].values[0]\n",
    "    val_acc = res[res[\"padding\"] == padding][\"val_acc\"].values[0]\n",
    "    plot_training(iters, loss_hist, train_acc, val_acc, axs[i])\n",
    "    axs[i].set_title(f\"padding = {padding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Małe różnice w modelach\n",
    "- Przy padding = 5 model robi się przetrenowany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liczba kanałów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_channels in tqdm_notebook.tqdm(n_channels_values):\n",
    "    padding = 1\n",
    "    n_convs = 2\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    try:\n",
    "        net = CustomNet(num_classes=50, input_size=48, n_convs=n_convs, kernel_size=kernel_size, stride=stride, padding=padding, n_channels=n_channels).to(device)\n",
    "    except:\n",
    "        continue\n",
    "    nets[(n_convs, kernel_size, stride, padding)] = net\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True, num_workers=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_hist, train_eval_hist, val_eval_hist = train_model(net, train_loader, val_loader, criterion, optimizer, get_accuracy, device, 15, verbose=False)\n",
    "    model_res = pd.DataFrame([[n_convs, kernel_size, stride, padding, n_channels,loss_hist, train_eval_hist, val_eval_hist, train_eval_hist[-1]-val_eval_hist[-1]]], columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\",\"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "    res = pd.concat([res, model_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for i, n_channels in enumerate(n_channels_values):\n",
    "    iters = [i for i in range(15)]\n",
    "    loss_hist = res[res[\"n_channels\"] == n_channels][\"train_loss\"].values[0]\n",
    "    train_acc = res[res[\"n_channels\"] == n_channels][\"train_acc\"].values[0]\n",
    "    val_acc = res[res[\"n_channels\"] == n_channels][\"val_acc\"].values[0]\n",
    "    plot_training(iters, loss_hist, train_acc, val_acc, axs[i])\n",
    "    axs[i].set_title(f\"n_channels = {n_channels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zwiększanie liczby kanałów przetrnowuje model\n",
    "- Duża różnica pomiędzy zbiorem walidacyjnym i treningowym\n",
    "- Większe wartości pozwalają na większą stabilność modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksperymenty na sieciach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\", \"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "nets = {}\n",
    "listofmodel=[[1,1,1,1,16],[2,5,1,1,16],[5,5,2,2,32],[2,3,2,2,32],[2,5,1,1,16]]\n",
    "for parameters in listofmodel:\n",
    
    "    n_convsact=parameters[0]  \n",
    "    kernel_size = parameters[1]\n",
    "    stride = parameters[2]\n",
    "    padding = parameters[3]\n",
    "    n_channels = parameters[4]\n",
    "    try:\n",
    "        net = CustomNet(num_classes=50, input_size=48, n_convs=n_convsact, kernel_size=kernel_size, stride=stride, padding=padding, n_channels=n_channels).to(device)\n",
    "    except:\n",
    "        continue\n",
    "    nets[(n_convsact, kernel_size, stride, padding)] = net\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True, num_workers=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_hist, train_eval_hist, val_eval_hist = train_model(net, train_loader, val_loader, criterion, optimizer, get_accuracy, device, 50, verbose=False)\n",
    "    model_res = pd.DataFrame([[n_convsact, kernel_size, stride, padding, n_channels,loss_hist, train_eval_hist, val_eval_hist, train_eval_hist[-1]-val_eval_hist[-1]]], columns=[\"n_convs\", \"kernel_size\", \"stride\", \"padding\", \"n_channels\",\"train_loss\", \"train_acc\", \"val_acc\", \"overfit\"])\n",
    "    torch.save(net.state_dict(), \"model\"+str(n_convsact)+str(kernel_size)+str(stride)+str(padding)+str(n_channels)+\".tar\")\n",
    "    res = pd.concat([res, model_res])\n",
    "res.to_csv('wyniki_modeli.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['acoustic', 'antenna', 'bacteria', 'battery', 'bean', 'beetle', 'bicycle', 'birch', 'bird', 'bomb', 'bread', 'bridge', 'camera', 'carbon', 'cat', 'corn', 'crab', 'crocodilian', 'echinoderm', 'egg', 'elephant', 'fish', 'flower', 'frog', 'fungus', 'gauge', 'hammer', 'icecream', 'kangaroo', 'memorial', 'monkey', 'motor', 'nest', 'palm', 'pizza', 'pot', 'printer', 'saw', 'snake', 'spice', 'spider', 'spoon', 'squash', 'swine', 'tea', 'tomato', 'towel', 'truck', 'turtle', 'worm']\n",
    "for net in nets:\n",
    "    balanced_accuracy_score, balanced_accuracy = get_balanced_accuracy(nets[net], classes=labels, data=val_loader, device=device)\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy_score}\")\n",
    "    final_acc = get_accuracy(nets[net], val_loader, device)\n",
    "    print(f\"Final accuracy: {final_acc}\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    label_score = {label: score for label, score in zip(labels, balanced_accuracy.numpy())}\n",
    "    label_score_sorted = {k: v for k, v in sorted(label_score.items(), key=lambda item: item[1])}\n",
    "    sns.barplot(x=list(label_score_sorted.keys()), y=list(label_score_sorted.values()), ax=ax, palette=\"viridis\")\n",
    "    plt.ylabel(\"Balanced accuracy\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    confusion_matrix = get_confusion_matrix(nets[net], val_loader, classes=labels, device=device) \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(confusion_matrix, xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
